#!/usr/bin/env python3
"""
å…³é”®ç‚¹æ¨¡å‹å½±å“åˆ†æè„šæœ¬
å¯¹æ¯”å½“å‰è¯„åˆ†ä¸å¼•å…¥å…³é”®ç‚¹æ£€æµ‹åçš„é¢„æµ‹è¯„åˆ†

ä½¿ç”¨æ–¹æ³•:
    python core/keypoint_impact_analysis.py /path/to/photos
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import numpy as np
import cv2
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from ultralytics import YOLO

# é…ç½®
MODEL_PATH = "models/cub200_keypoint_resnet50.pth"
YOLO_MODEL_PATH = "models/yolo11m-seg.pt"
IMG_SIZE = 416
VISIBILITY_THRESHOLD = 0.5  # å¯è§æ€§é˜ˆå€¼


class PartLocalizer(nn.Module):
    """å…³é”®ç‚¹å®šä½æ¨¡å‹"""
    def __init__(self, backbone='resnet50', num_parts=3, hidden_dim=512, dropout=0.2):
        super().__init__()
        self.num_parts = num_parts
        self.backbone = models.resnet50(weights=None)
        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Identity()

        self.head = nn.Sequential(
            nn.Linear(in_features, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
        )
        self.coord_head = nn.Linear(hidden_dim // 2, num_parts * 2)
        self.vis_head = nn.Linear(hidden_dim // 2, num_parts)

    def forward(self, x):
        features = self.head(self.backbone(x))
        coords = torch.sigmoid(self.coord_head(features)).view(-1, self.num_parts, 2)
        vis = torch.sigmoid(self.vis_head(features))
        return coords, vis


def load_keypoint_model(device):
    """åŠ è½½å…³é”®ç‚¹æ¨¡å‹"""
    print(f"ğŸ“¦ åŠ è½½å…³é”®ç‚¹æ¨¡å‹...")
    model = PartLocalizer()
    checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    model.to(device)
    model.eval()
    return model


def calculate_eye_sharpness(image, keypoint, radius=25):
    """è®¡ç®—çœ¼ç›åŒºåŸŸé”åº¦"""
    h, w = image.shape[:2]
    x, y = int(keypoint[0] * w), int(keypoint[1] * h)
    
    # è¾¹ç•Œæ£€æŸ¥
    x1, y1 = max(0, x - radius), max(0, y - radius)
    x2, y2 = min(w, x + radius), min(h, y + radius)
    
    if x2 - x1 < 10 or y2 - y1 < 10:
        return 0.0
    
    eye_region = image[y1:y2, x1:x2]
    if eye_region.size == 0:
        return 0.0
    
    # è½¬ç°åº¦è®¡ç®— Laplacian æ–¹å·®
    if len(eye_region.shape) == 3:
        gray = cv2.cvtColor(eye_region, cv2.COLOR_RGB2GRAY)
    else:
        gray = eye_region
    
    laplacian = cv2.Laplacian(gray, cv2.CV_64F)
    sharpness = laplacian.var()
    
    return sharpness


def analyze_image(jpg_path, yolo_model, keypoint_model, device, transform):
    """åˆ†æå•å¼ å›¾ç‰‡"""
    result = {
        'file': os.path.basename(jpg_path),
        'has_bird': False,
        'left_eye_vis': 0.0,
        'right_eye_vis': 0.0,
        'beak_vis': 0.0,
        'both_eyes_hidden': False,
        'eye_sharpness': 0.0,
        'visible_eye': None,
        'impact': 'none',
        'reason': ''
    }
    
    # YOLO æ£€æµ‹
    yolo_results = yolo_model(jpg_path, verbose=False)
    
    bird_detected = False
    bird_crop = None
    
    for r in yolo_results:
        if r.boxes is not None and len(r.boxes) > 0:
            for i, cls in enumerate(r.boxes.cls):
                if int(cls) == 14:  # bird class
                    bird_detected = True
                    # è·å–è¾¹ç•Œæ¡†
                    box = r.boxes.xyxy[i].cpu().numpy().astype(int)
                    x1, y1, x2, y2 = box
                    
                    # è¯»å–å›¾ç‰‡å¹¶è£å‰ª
                    img = cv2.imread(jpg_path)
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    bird_crop = img[y1:y2, x1:x2]
                    break
        if bird_detected:
            break
    
    if not bird_detected or bird_crop is None:
        return result
    
    result['has_bird'] = True
    
    # å…³é”®ç‚¹æ£€æµ‹
    pil_crop = Image.fromarray(bird_crop)
    tensor = transform(pil_crop).unsqueeze(0).to(device)
    
    with torch.no_grad():
        coords, vis = keypoint_model(tensor)
    
    coords = coords[0].cpu().numpy()
    vis = vis[0].cpu().numpy()
    
    result['left_eye_vis'] = float(vis[0])
    result['right_eye_vis'] = float(vis[1])
    result['beak_vis'] = float(vis[2])
    
    # åˆ¤æ–­åŒçœ¼å¯è§æ€§
    left_visible = vis[0] >= VISIBILITY_THRESHOLD
    right_visible = vis[1] >= VISIBILITY_THRESHOLD
    
    result['both_eyes_hidden'] = not left_visible and not right_visible
    
    # è®¡ç®—çœ¼ç›é”åº¦
    if left_visible and right_visible:
        # ä¸¤çœ¼éƒ½å¯è§ï¼Œå–å¹³å‡
        left_sharp = calculate_eye_sharpness(bird_crop, coords[0])
        right_sharp = calculate_eye_sharpness(bird_crop, coords[1])
        result['eye_sharpness'] = (left_sharp + right_sharp) / 2
        result['visible_eye'] = 'both'
    elif left_visible:
        result['eye_sharpness'] = calculate_eye_sharpness(bird_crop, coords[0])
        result['visible_eye'] = 'left'
    elif right_visible:
        result['eye_sharpness'] = calculate_eye_sharpness(bird_crop, coords[1])
        result['visible_eye'] = 'right'
    else:
        result['visible_eye'] = 'none'
    
    # åˆ¤æ–­å½±å“
    if result['both_eyes_hidden']:
        result['impact'] = 'downgrade'
        result['reason'] = 'åŒçœ¼ä¸å¯è§(è§’åº¦ä¸ä½³)'
    elif result['eye_sharpness'] > 500:  # é«˜é”åº¦é˜ˆå€¼ï¼ˆéœ€è¦æ ¹æ®å®é™…è°ƒæ•´ï¼‰
        result['impact'] = 'potential_upgrade'
        result['reason'] = f"çœ¼ç›é”åº¦é«˜({result['eye_sharpness']:.0f})"
    else:
        result['impact'] = 'none'
        result['reason'] = 'æ­£å¸¸'
    
    return result


def find_jpg_files(directory):
    """æŸ¥æ‰¾ç›®å½•ä¸­çš„ JPG æ–‡ä»¶ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰"""
    jpg_files = []
    for root, dirs, files in os.walk(directory):
        for f in files:
            if f.lower().endswith(('.jpg', '.jpeg')):
                jpg_files.append(os.path.join(root, f))
    return jpg_files


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python core/keypoint_impact_analysis.py /path/to/photos")
        sys.exit(1)
    
    photo_dir = sys.argv[1]
    
    print("\n" + "="*70)
    print("ğŸ” å…³é”®ç‚¹æ¨¡å‹å½±å“åˆ†æ")
    print("="*70)
    
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    keypoint_model = load_keypoint_model(device)
    print(f"ğŸ“¦ åŠ è½½ YOLO æ¨¡å‹...")
    yolo_model = YOLO(YOLO_MODEL_PATH)
    
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # æŸ¥æ‰¾ JPG æ–‡ä»¶
    # å…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢ RAW
    print(f"\nğŸ“‚ æ‰«æç›®å½•: {photo_dir}")
    
    # ä¼˜å…ˆä½¿ç”¨å·²è½¬æ¢çš„ JPGï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    jpg_files = find_jpg_files(photo_dir)
    
    if not jpg_files:
        print("âš ï¸  æœªæ‰¾åˆ° JPG æ–‡ä»¶")
        print("   è¯·å…ˆè¿è¡Œ SuperPicky å¤„ç†ç›®å½•ä»¥ç”Ÿæˆä¸´æ—¶ JPG")
        sys.exit(1)
    
    print(f"ğŸ“ æ‰¾åˆ° {len(jpg_files)} ä¸ª JPG æ–‡ä»¶")
    
    # åˆ†ææ¯å¼ å›¾ç‰‡
    print("\n" + "-"*70)
    print("å¼€å§‹åˆ†æ...")
    print("-"*70)
    
    results = []
    stats = {
        'total': 0,
        'has_bird': 0,
        'both_eyes_hidden': 0,
        'potential_upgrade': 0,
        'no_change': 0
    }
    
    for i, jpg_path in enumerate(jpg_files, 1):
        print(f"\r[{i}/{len(jpg_files)}] åˆ†æä¸­...", end='', flush=True)
        
        try:
            result = analyze_image(jpg_path, yolo_model, keypoint_model, device, transform)
            results.append(result)
            
            stats['total'] += 1
            if result['has_bird']:
                stats['has_bird'] += 1
                if result['both_eyes_hidden']:
                    stats['both_eyes_hidden'] += 1
                elif result['impact'] == 'potential_upgrade':
                    stats['potential_upgrade'] += 1
                else:
                    stats['no_change'] += 1
        except Exception as e:
            print(f"\nâš ï¸  åˆ†æå¤±è´¥: {os.path.basename(jpg_path)} - {e}")
    
    print("\n")
    
    # è¾“å‡ºç»Ÿè®¡
    print("="*70)
    print("ğŸ“Š å½±å“åˆ†æç»Ÿè®¡")
    print("="*70)
    print(f"æ€»æ–‡ä»¶æ•°: {stats['total']}")
    print(f"æ£€æµ‹åˆ°é¸Ÿ: {stats['has_bird']}")
    print(f"")
    print(f"ğŸ”» ä¼šé™çº§çš„ç…§ç‰‡ (åŒçœ¼ä¸å¯è§): {stats['both_eyes_hidden']} ({stats['both_eyes_hidden']/max(1,stats['has_bird'])*100:.1f}%)")
    print(f"ğŸ”º å¯èƒ½å‡çº§çš„ç…§ç‰‡ (çœ¼ç›é”åº¦é«˜): {stats['potential_upgrade']} ({stats['potential_upgrade']/max(1,stats['has_bird'])*100:.1f}%)")
    print(f"â– ä¸å—å½±å“çš„ç…§ç‰‡: {stats['no_change']} ({stats['no_change']/max(1,stats['has_bird'])*100:.1f}%)")
    
    # è¾“å‡ºå—å½±å“çš„ç…§ç‰‡è¯¦æƒ…
    print("\n" + "-"*70)
    print("ğŸ“‹ å—å½±å“çš„ç…§ç‰‡è¯¦æƒ…")
    print("-"*70)
    
    affected = [r for r in results if r['impact'] != 'none']
    
    if not affected:
        print("âœ… æ²¡æœ‰ç…§ç‰‡ä¼šå—åˆ°æ˜¾è‘—å½±å“")
    else:
        print(f"\n{'æ–‡ä»¶å':<40} {'å½±å“':<12} {'åŸå› ':<30}")
        print("-"*82)
        for r in affected:
            impact_str = "ğŸ”»é™çº§" if r['impact'] == 'downgrade' else "ğŸ”ºå¯èƒ½å‡çº§"
            print(f"{r['file']:<40} {impact_str:<12} {r['reason']:<30}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_path = os.path.join(photo_dir, "_keypoint_impact_report.json")
    report = {
        'timestamp': datetime.now().isoformat(),
        'directory': photo_dir,
        'stats': stats,
        'affected_files': affected,
        'all_results': results
    }
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    print("="*70)


if __name__ == "__main__":
    main()
